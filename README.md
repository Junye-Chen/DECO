# DECO
The official implement of "Decouple and Couple: Exploiting Prior Knowledge for Visible Video Watermark Removal".
*This file is generated by AI and may contain errors.*

[![paper](https://img.shields.io/badge/IEEE-Paper-<COLOR>.svg)](https://ieeexplore.ieee.org/abstract/document/10871924)

## Overview

DECO is a multi-stage video watermark removal framework. It removes watermarks from videos by jointly training:

- a Intrinsic Background Image Prediction (IBIP) that restores watermark-free video frames,
- an image generator with vector-quantised priors for fine-grained texture injection, and
- a video restore backbone (MSVT) that leverages long-range temporal context.

The pipeline supports large-scale multi-GPU training, custom watermark synthesis, and evaluation on DAVIS and YouTube-VOS benchmarks with PSNR, SSIM, LPIPS, RMSE, and VFID metrics.

## Repository Layout

- `train.py`: entry point for distributed training with configurable stages (image, CPN, full video).
- `evaluate.py`: runs evaluation/inference with pre-trained checkpoints and optional video export.
- `configs/`: JSON config files for the different training stages.
- `core/`: dataloaders, losses, schedulers, metrics, dist utils, trainers, and watermark generators.
- `model/`: network definitions (CPN, MSVT generator, VQ branches, mask utilities).
- `datasets/`: example JSON splits for DAVIS and YouTube-VOS.
- `utilss/`: IO helpers, downloading utilities, image/metric helpers.

## Environment Setup

**Prerequisites**

- Python 3.8+
- CUDA-capable GPU(s) with recent NVIDIA drivers
- FFmpeg on the system `PATH` for MP4 export (`imageio.mimwrite`)

Dependency installation (replace the CUDA wheel as needed):

```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
pip install numpy scipy scikit-image opencv-python pillow imageio imageio-ffmpeg lpips einops matplotlib tqdm tensorboard requests
```

Optional backends (`mc`, `lmdb`) can be installed if you switch the `FileClient` to non-disk storage.

## Data Preparation

<table>
<thead>
  <tr>
    <th>Dataset</th>
    <th>YouTube-VOS</th>
    <th>DAVIS</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Description</td>
    <td>For training (3,471) and evaluation (508)</td>
    <td>For evaluation (50 in 90)</td>
  <tr>
    <td>Images</td>
    <td> [<a href="https://competitions.codalab.org/competitions/19544#participate-get-data">Official Link</a>] (Download train and test all frames) </td>
    <td> [<a href="https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-trainval-480p.zip">Official Link</a>] (2017, 480p, TrainVal) </td>
  </tr>
  <tr>
    <td>Masks</td>
    <td colspan="2"> [<a href="https://drive.google.com/file/d/1dFTneS_zaJAHjglxU10gYzr1-xALgHa4/view?usp=sharing">Google Drive</a>] [<a href="https://pan.baidu.com/s/1JC-UKmlQfjhVtD81196cxA?pwd=87e3">Baidu Disk</a>] (For reproducing paper results; provided in <a href="https://arxiv.org/abs/2309.03897">ProPainter</a> paper) </td>
  </tr>
</tbody>
</table>

The training and test split files are provided in `datasets/<dataset_name>`. For each dataset, you should place `JPEGImages` to `datasets/<dataset_name>`. Resize all video frames to size `432x240` for training. Unzip downloaded mask files to `datasets`.

The `datasets` directory structure will be arranged as: (**Note**: please check it carefully)
```
datasets
   |- davis
      |- JPEGImages_432_240
         |- <video_name>
            |- 00000.jpg
            |- 00001.jpg
      |- test_masks
         |- <video_name>
            |- 00000.png
            |- 00001.png   
      |- train.json
      |- test.json
   |- youtube-vos
      |- JPEGImages_432_240
         |- <video_name>
            |- 00000.jpg
            |- 00001.jpg
      |- test_masks
         |- <video_name>
            |- 00000.png
            |- 00001.png
      |- train.json
      |- test.json   
```


## Quick Start
### Training Pipeline

The project supports stage-wise training; each stage has its own config in `configs/`:

1. **Image codebook pre-training**: `configs/train_imgVQ.json` trains the vector-quantised image branch.
2. **Image generator fine-tuning**: `configs/train_img.json` refines the image reconstruction model using precomputed codebooks.
3. **Intrinsic Background Image Prediction pre-training**: `configs/train_cpn.json` learns the IBIP component (single-frame clean estimation).
4. **Full video model**: `configs/train_full.json` jointly optimises MSVT with frozen auxiliary branches.

Adjust data roots, batch size, learning rates, and schedule parameters per stage before launching `train.py`.

**Launch training** (multi-GPU example; modifies config in place):

```bash
python train.py -c configs/train_full.json -p 23490
```

`train.py` spawns one process per available GPU; set `CUDA_VISIBLE_DEVICES` to control device usage. Trainer checkpoints, logs, TensorBoard events, and snapshots accumulate under `experiments_model/<net>_<config_name>/`.



### Run evaluation

```bash
python evaluate.py \
    --dataset davis \
    --name davis \
    --video_root path/to/davis/JPEGImages \
    --mask_root path/to/generated/masks \
    --pretrain weights/msvt_final.pth \
    --save_results --save_video
```

Results (frames, videos, metrics) are written to `results_eval/<dataset>_rs_<stride>_nl_<neighbor>_*`.

## Citation

If you find our repo useful for your research, please consider citing our paper:

```bibtex
@ARTICLE{10871924,
  author={Chen, Junye and Fang, Chaowei and Li, Jichang and Leng, Yicheng and Li, Guanbin},
  journal={IEEE Transactions on Image Processing}, 
  title={Decouple and Couple: Exploiting Prior Knowledge for Visible Video Watermark Removal}, 
  year={2025},
  volume={34},
  number={},
  pages={1192-1203},
  keywords={Watermarking;Feature extraction;Image restoration;Visualization;Data mining;Semantics;Interference;Commonsense reasoning;Adaptation models;Noise;Visible video watermark removal;prior knowledge extraction;temporal modeling},
  doi={10.1109/TIP.2025.3534033}}

```


## Acknowledgement

This code is based on [ProPainter](https://github.com/sczhou/ProPainter). Some code are brought from [FeMaSR](https://github.com/chaofengc/FeMaSR). Thanks for their awesome works. 


## License

This project is released under the Apache 2.0 license (see `LICENSE`).